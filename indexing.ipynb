{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_5681/3274793692.py:8: DeprecationWarning: Call to deprecated function (or staticmethod) started. (use pt.java.started() instead) -- Deprecated since version 0.11.0.\n",
      "  if not pt.started(): pt.init()\n",
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n",
      "/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_5681/3274793692.py:8: DeprecationWarning: Call to deprecated method pt.init(). Deprecated since version 0.11.0.\n",
      "java is now started automatically with default settings. To force initialisation early, run:\n",
      "pt.java.init() # optional, forces java initialisation\n",
      "  if not pt.started(): pt.init()\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pyterrier as pt\n",
    "if not pt.started(): pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IDX = \"indexes\"\n",
    "\n",
    "# load queries (strip punctuation) and qrels\n",
    "qs = pd.read_csv(\"data/train_queries.csv\", sep=\"\\t\", names=[\"qid\", \"query\"], header=0)\n",
    "qs['query'] = qs['query'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "qrels = pd.read_csv(\"data/train_qrels.csv\", sep=\"\\t\")\n",
    "docs = pd.read_json(\"data/docs.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/christianjensen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "docs['body_word_count'] = docs['body'].apply(lambda x: len(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab54b47a76f46a18950e69e15dc7d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total Builds:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:47:19.722 [ForkJoinPool-1-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "07:50:20.838 [ForkJoinPool-1-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 302 empty documents\n",
      "07:50:58.283 [ForkJoinPool-2-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "07:53:44.009 [ForkJoinPool-2-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 303 empty documents\n",
      "07:54:23.326 [ForkJoinPool-3-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "07:57:26.171 [ForkJoinPool-3-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 302 empty documents\n",
      "07:58:00.697 [ForkJoinPool-4-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "08:00:42.451 [ForkJoinPool-4-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 303 empty documents\n",
      "08:02:47.568 [ForkJoinPool-5-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "08:03:53.619 [ForkJoinPool-5-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "08:05:48.884 [ForkJoinPool-6-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "08:06:48.980 [ForkJoinPool-6-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "08:09:02.363 [ForkJoinPool-7-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "08:10:02.209 [ForkJoinPool-7-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "08:11:59.479 [ForkJoinPool-8-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "08:12:51.021 [ForkJoinPool-8-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "08:13:27.014 [ForkJoinPool-9-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "08:16:27.668 [ForkJoinPool-9-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 302 empty documents\n",
      "08:17:01.672 [ForkJoinPool-10-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "08:19:54.684 [ForkJoinPool-10-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 303 empty documents\n",
      "08:20:33.329 [ForkJoinPool-11-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "08:23:24.687 [ForkJoinPool-11-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 302 empty documents\n",
      "08:23:59.311 [ForkJoinPool-12-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "08:26:32.691 [ForkJoinPool-12-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 303 empty documents\n",
      "08:28:36.800 [ForkJoinPool-13-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "08:29:40.457 [ForkJoinPool-13-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "08:31:37.694 [ForkJoinPool-14-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "08:32:38.267 [ForkJoinPool-14-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "08:34:51.839 [ForkJoinPool-15-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "08:35:50.992 [ForkJoinPool-15-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "08:37:49.706 [ForkJoinPool-16-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "08:38:43.127 [ForkJoinPool-16-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "08:39:19.718 [ForkJoinPool-17-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "08:42:14.083 [ForkJoinPool-17-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 302 empty documents\n",
      "08:42:47.847 [ForkJoinPool-18-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "08:45:28.024 [ForkJoinPool-18-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 303 empty documents\n",
      "08:46:13.254 [ForkJoinPool-19-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "08:49:21.086 [ForkJoinPool-19-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 302 empty documents\n",
      "08:49:58.288 [ForkJoinPool-20-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "08:52:56.482 [ForkJoinPool-20-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 303 empty documents\n",
      "08:55:10.963 [ForkJoinPool-21-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "08:56:19.257 [ForkJoinPool-21-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "08:58:27.320 [ForkJoinPool-22-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "08:59:29.504 [ForkJoinPool-22-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "09:01:49.510 [ForkJoinPool-23-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "09:02:54.200 [ForkJoinPool-23-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "09:04:58.082 [ForkJoinPool-24-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "09:05:53.603 [ForkJoinPool-24-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "09:06:33.378 [ForkJoinPool-25-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "09:09:34.459 [ForkJoinPool-25-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 302 empty documents\n",
      "09:12:39.351 [ForkJoinPool-26-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "09:15:24.868 [ForkJoinPool-26-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 303 empty documents\n",
      "09:16:03.945 [ForkJoinPool-27-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "09:19:02.817 [ForkJoinPool-27-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 302 empty documents\n",
      "09:19:38.029 [ForkJoinPool-28-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "09:22:14.615 [ForkJoinPool-28-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 303 empty documents\n",
      "09:24:24.818 [ForkJoinPool-29-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "09:25:34.076 [ForkJoinPool-29-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "09:27:46.923 [ForkJoinPool-30-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "09:28:54.127 [ForkJoinPool-30-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "09:31:36.426 [ForkJoinPool-31-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "09:32:54.812 [ForkJoinPool-31-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "09:35:29.298 [ForkJoinPool-32-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "09:36:32.351 [ForkJoinPool-32-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "09:37:17.507 [ForkJoinPool-33-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "09:40:47.999 [ForkJoinPool-33-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 302 empty documents\n",
      "09:41:29.191 [ForkJoinPool-34-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "09:44:39.456 [ForkJoinPool-34-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 303 empty documents\n",
      "09:45:18.262 [ForkJoinPool-35-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "09:48:16.521 [ForkJoinPool-35-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 302 empty documents\n",
      "09:48:51.504 [ForkJoinPool-36-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D3037951) - further warnings are suppressed\n",
      "09:51:33.695 [ForkJoinPool-36-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 303 empty documents\n",
      "09:53:54.903 [ForkJoinPool-37-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "09:55:13.514 [ForkJoinPool-37-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "09:57:26.663 [ForkJoinPool-38-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "09:58:32.514 [ForkJoinPool-38-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "10:00:57.572 [ForkJoinPool-39-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "10:02:14.148 [ForkJoinPool-39-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n",
      "10:04:29.313 [ForkJoinPool-40-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (D2488975) - further warnings are suppressed\n",
      "10:05:33.645 [ForkJoinPool-40-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 1 empty documents\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     52\u001b[39m                 pbar.update(\u001b[32m1\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# quick summary\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m bt = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_times\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBuild times summary (s):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(bt.describe().T[[\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m]])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/search-engines/venv/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/search-engines/venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/search-engines/venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/search-engines/venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:677\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    675\u001b[39m lengths = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll arrays must be of the same length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "NO_STEM = pt.TerrierStemmer.none\n",
    "\n",
    "configs = {\n",
    "    \"full_index\": {\n",
    "        \"stopwords\": None,\n",
    "        \"stemmer\"  : NO_STEM,\n",
    "    },\n",
    "    \"stopwords_removed\": {\n",
    "        \"stemmer\": NO_STEM,\n",
    "    },\n",
    "    \"stemming_only\": {\n",
    "        \"stopwords\": None,\n",
    "    },\n",
    "    \"stopwords_and_stemming\": {\n",
    "        # defaults\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "N_BUILDS = 5\n",
    "build_times = {name: [] for name in configs}\n",
    "build_times.update({name + \"_trimmed\": [] for name in configs})\n",
    "\n",
    "lower = docs['body_word_count'].quantile(0.01)\n",
    "upper = docs['body_word_count'].quantile(0.99)\n",
    "\n",
    "total_runs = N_BUILDS * 2 * len(configs)\n",
    "\n",
    "with tqdm(total=total_runs, desc=\"Total Builds\") as pbar:\n",
    "    for run in range(N_BUILDS):\n",
    "        for trimmed in (False, True):\n",
    "            for name, opts in configs.items():\n",
    "                run_name = f\"{name}_trimmed\" if trimmed else f\"{name}_full\"\n",
    "                abs_idx = os.path.abspath(os.path.join(BASE_IDX, run_name))\n",
    "                \n",
    "                # wipe & rebuild\n",
    "                if os.path.isdir(abs_idx): shutil.rmtree(abs_idx)\n",
    "                os.makedirs(abs_idx, exist_ok=True)\n",
    "\n",
    "                idxer = pt.IterDictIndexer(abs_idx, text_attrs=[\"body\", \"title\"], fields=True, **opts)\n",
    "                t0 = time.perf_counter()\n",
    "                if trimmed:\n",
    "                    idx_ref = idxer.index(docs[\n",
    "                        (docs['body_word_count'] > lower) &\n",
    "                        (docs['body_word_count'] < upper)\n",
    "                    ].to_dict(orient='records'))\n",
    "                else:\n",
    "                    idx_ref = idxer.index(docs.to_dict(orient='records'))\n",
    "                \n",
    "                build_times[run_name].append(time.perf_counter() - t0)\n",
    "                pbar.update(1)\n",
    "# quick summary\n",
    "bt = pd.DataFrame(build_times)\n",
    "print(\"Build times summary (s):\")\n",
    "print(bt.describe().T[[\"mean\",\"std\",\"min\",\"max\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build times summary (s):\n",
      "                                      mean        std         min         max\n",
      "full_index                      224.361024  17.764385  210.837642  255.471691\n",
      "full_index_trimmed              200.103897  12.415554  187.870443  219.609771\n",
      "stopwords_removed               206.471247  14.796586  194.001272  231.351704\n",
      "stopwords_removed_trimmed       188.053850  11.411245  175.334452  198.935515\n",
      "stemming_only                   219.873281   8.411347  209.852369  232.689631\n",
      "stemming_only_trimmed           210.452104  20.494514  192.553874  240.537750\n",
      "stopwords_and_stemming          197.518740  10.622081  187.514076  215.243656\n",
      "stopwords_and_stemming_trimmed  187.346538  20.582123  168.839451  217.388120\n"
     ]
    }
   ],
   "source": [
    "bt = pd.DataFrame(build_times)\n",
    "print(\"Build times summary (s):\")\n",
    "print(bt.describe().T[[\"mean\",\"std\",\"min\",\"max\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_index: mean=224.4s, 95% CI=(202.3, 246.4)\n",
      "full_index_trimmed: mean=200.1s, 95% CI=(184.7, 215.5)\n",
      "stopwords_removed: mean=206.5s, 95% CI=(188.1, 224.8)\n",
      "stopwords_removed_trimmed: mean=188.1s, 95% CI=(173.9, 202.2)\n",
      "stemming_only: mean=219.9s, 95% CI=(209.4, 230.3)\n",
      "stemming_only_trimmed: mean=210.5s, 95% CI=(185.0, 235.9)\n",
      "stopwords_and_stemming: mean=197.5s, 95% CI=(184.3, 210.7)\n",
      "stopwords_and_stemming_trimmed: mean=187.3s, 95% CI=(161.8, 212.9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "for name, times in build_times.items():\n",
    "    arr = np.array(times)\n",
    "    n = len(arr)\n",
    "    mean = arr.mean()\n",
    "    sem  = stats.sem(arr)\n",
    "    t_crit = stats.t.ppf(0.975, df=n-1)\n",
    "    h = t_crit * sem\n",
    "    ci = (mean - h, mean + h)\n",
    "    print(f\"{name}: mean={mean:.1f}s, 95% CI=({ci[0]:.1f}, {ci[1]:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_index                     vs full_index_trimmed             → p=0.040, holm p=0.843 ✗, Cohen’s d=1.58\n",
      "full_index                     vs stopwords_removed              → p=0.123, holm p=1.000 ✗, Cohen’s d=1.09\n",
      "full_index                     vs stopwords_removed_trimmed      → p=0.007, holm p=0.180 ✗, Cohen’s d=2.43\n",
      "full_index                     vs stemming_only                  → p=0.629, holm p=1.000 ✗, Cohen’s d=0.32\n",
      "full_index                     vs stemming_only_trimmed          → p=0.285, holm p=1.000 ✗, Cohen’s d=0.73\n",
      "full_index                     vs stopwords_and_stemming         → p=0.025, holm p=0.546 ✗, Cohen’s d=1.83\n",
      "full_index                     vs stopwords_and_stemming_trimmed → p=0.016, holm p=0.409 ✗, Cohen’s d=1.93\n",
      "full_index_trimmed             vs stopwords_removed              → p=0.483, holm p=1.000 ✗, Cohen’s d=-0.47\n",
      "full_index_trimmed             vs stopwords_removed_trimmed      → p=0.149, holm p=1.000 ✗, Cohen’s d=1.01\n",
      "full_index_trimmed             vs stemming_only                  → p=0.021, holm p=0.491 ✗, Cohen’s d=-1.86\n",
      "full_index_trimmed             vs stemming_only_trimmed          → p=0.368, holm p=1.000 ✗, Cohen’s d=-0.61\n",
      "full_index_trimmed             vs stopwords_and_stemming         → p=0.733, holm p=1.000 ✗, Cohen’s d=0.22\n",
      "full_index_trimmed             vs stopwords_and_stemming_trimmed → p=0.276, holm p=1.000 ✗, Cohen’s d=0.75\n",
      "stopwords_removed              vs stopwords_removed_trimmed      → p=0.061, holm p=1.000 ✗, Cohen’s d=1.39\n",
      "stopwords_removed              vs stemming_only                  → p=0.126, holm p=1.000 ✗, Cohen’s d=-1.11\n",
      "stopwords_removed              vs stemming_only_trimmed          → p=0.735, holm p=1.000 ✗, Cohen’s d=-0.22\n",
      "stopwords_removed              vs stopwords_and_stemming         → p=0.307, holm p=1.000 ✗, Cohen’s d=0.70\n",
      "stopwords_removed              vs stopwords_and_stemming_trimmed → p=0.134, holm p=1.000 ✗, Cohen’s d=1.07\n",
      "stopwords_removed_trimmed      vs stemming_only                  → p=0.001, holm p=0.037 ✓, Cohen’s d=-3.17\n",
      "stopwords_removed_trimmed      vs stemming_only_trimmed          → p=0.075, holm p=1.000 ✗, Cohen’s d=-1.35\n",
      "stopwords_removed_trimmed      vs stopwords_and_stemming         → p=0.212, holm p=1.000 ✗, Cohen’s d=-0.86\n",
      "stopwords_removed_trimmed      vs stopwords_and_stemming_trimmed → p=0.949, holm p=1.000 ✗, Cohen’s d=0.04\n",
      "stemming_only                  vs stemming_only_trimmed          → p=0.383, holm p=1.000 ✗, Cohen’s d=0.60\n",
      "stemming_only                  vs stopwords_and_stemming         → p=0.007, holm p=0.180 ✗, Cohen’s d=2.33\n",
      "stemming_only                  vs stopwords_and_stemming_trimmed → p=0.020, holm p=0.489 ✗, Cohen’s d=2.07\n",
      "stemming_only_trimmed          vs stopwords_and_stemming         → p=0.257, holm p=1.000 ✗, Cohen’s d=0.79\n",
      "stemming_only_trimmed          vs stopwords_and_stemming_trimmed → p=0.113, holm p=1.000 ✗, Cohen’s d=1.12\n",
      "stopwords_and_stemming         vs stopwords_and_stemming_trimmed → p=0.364, holm p=1.000 ✗, Cohen’s d=0.62\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def cohens_d(x, y):\n",
    "    \"\"\"Compute Cohen's d for two independent samples.\"\"\"\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    n1, n2 = len(x), len(y)\n",
    "    s1, s2 = x.var(ddof=1), y.var(ddof=1)\n",
    "    s_pooled = np.sqrt(((n1-1)*s1 + (n2-1)*s2) / (n1+n2-2))\n",
    "    return (x.mean() - y.mean()) / s_pooled\n",
    "\n",
    "# 1) Pairwise Welch tests\n",
    "pairs = list(itertools.combinations(build_times.keys(), 2))\n",
    "pvals = [stats.ttest_ind(build_times[a], build_times[b], equal_var=False)[1]\n",
    "         for a, b in pairs]\n",
    "\n",
    "# 2) Holm–Bonferroni correction\n",
    "reject_holm, pvals_holm, _, _ = multipletests(pvals, alpha=0.05, method='holm')\n",
    "\n",
    "# 3) Compute Cohen's d for each pair and report\n",
    "for (a, b), p_raw, p_holm, sig in zip(pairs, pvals, pvals_holm, reject_holm):\n",
    "    d = cohens_d(build_times[a], build_times[b])\n",
    "    status = \"✓\" if sig else \"✗\"\n",
    "    print(f\"{a:30s} vs {b:30s} → p={p_raw:.3f}, holm p={p_holm:.3f} {status}, Cohen’s d={d:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not_trimmed: Shapiro-Wilk p = 0.286\n",
      "trimmed: Shapiro-Wilk p = 0.441\n",
      "\n",
      "All non-trimmed vs trimmed → Welch p=0.0074, d=0.90\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# collect lists\n",
    "non_trimmed = np.concatenate([\n",
    "    build_times['full_index'],\n",
    "    build_times['stopwords_removed'],\n",
    "    build_times['stemming_only'],\n",
    "    build_times['stopwords_and_stemming']\n",
    "])\n",
    "trimmed = np.concatenate([\n",
    "    build_times['full_index_trimmed'],\n",
    "    build_times['stopwords_removed_trimmed'],\n",
    "    build_times['stemming_only_trimmed'],\n",
    "    build_times['stopwords_and_stemming_trimmed']\n",
    "])\n",
    "\n",
    "\n",
    "for name, arr in [('not_trimmed', non_trimmed), ('trimmed', trimmed)]:\n",
    "    stat, p = shapiro(arr)\n",
    "    print(f\"{name}: Shapiro-Wilk p = {p:.3f}\")\n",
    "\n",
    "# Welch’s independent t-test\n",
    "tstat, pval = stats.ttest_ind(non_trimmed, trimmed, equal_var=False)\n",
    "\n",
    "d = cohens_d(non_trimmed, trimmed)\n",
    "print(f\"\\nAll non-trimmed vs trimmed → Welch p={pval:.4f}, d={d:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full: Shapiro-Wilk p = 0.236\n",
      "stopwords: Shapiro-Wilk p = 0.350\n",
      "stemming: Shapiro-Wilk p = 0.765\n",
      "\n",
      "full       vs stopwords  → Welch p=0.0744, Cohen's d=0.85\n",
      "full       vs stemming   → Welch p=0.7132, Cohen's d=-0.17\n"
     ]
    }
   ],
   "source": [
    "full = np.concatenate([build_times['full_index'], build_times['full_index_trimmed']])\n",
    "stopwords = np.concatenate([build_times['stopwords_removed'], build_times['stopwords_removed_trimmed']])\n",
    "stemming = np.concatenate([build_times['stemming_only'], build_times['stemming_only_trimmed']])\n",
    "\n",
    "for name, arr in [('full', full), ('stopwords', stopwords), ('stemming', stemming)]:\n",
    "    stat, p = shapiro(arr)\n",
    "    print(f\"{name}: Shapiro-Wilk p = {p:.3f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Compare full vs stopwords, and full vs stemming\n",
    "comparisons = [('full', full), ('stopwords', stopwords), ('stemming', stemming)]\n",
    "for name1, data1 in comparisons:\n",
    "    if name1 == 'full':\n",
    "        for name2, data2 in comparisons[1:]:\n",
    "            tstat, pval = stats.ttest_ind(data1, data2, equal_var=False)\n",
    "            d = cohens_d(data1, data2)\n",
    "            print(f\"{name1:10s} vs {name2:10s} → Welch p={pval:.4f}, Cohen's d={d:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83146634d39f48b7bf81804aa964a756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[QUERY] stemming_only:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b14538df70493fa6c60c61469acb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[QUERY] stopwords_removed_trimmed:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:21:09.276 [main] WARN org.terrier.structures.FSADocumentIndex -- This index has fields, but FSADocumentIndex is used (which stores fields lengths on disk); If using field-based models such as BM25F, change to index.document.class in the index  properties file to FSAFieldDocumentIndex or FSADocumentIndexInMemFields to support efficient retrieval. If you don't use (e.g.) BM25F, this warning can be ignored\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6855ad90d19f4fc489ad80058289f849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[QUERY] stopwords_and_stemming_trimmed:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:23:50.088 [main] WARN org.terrier.structures.FSADocumentIndex -- This index has fields, but FSADocumentIndex is used (which stores fields lengths on disk); If using field-based models such as BM25F, change to index.document.class in the index  properties file to FSAFieldDocumentIndex or FSADocumentIndexInMemFields to support efficient retrieval. If you don't use (e.g.) BM25F, this warning can be ignored\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4f76a6b1cb452aa4a8ab84a2aadbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[QUERY] stemming_only_trimmed:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:26:51.087 [main] WARN org.terrier.structures.FSADocumentIndex -- This index has fields, but FSADocumentIndex is used (which stores fields lengths on disk); If using field-based models such as BM25F, change to index.document.class in the index  properties file to FSAFieldDocumentIndex or FSADocumentIndexInMemFields to support efficient retrieval. If you don't use (e.g.) BM25F, this warning can be ignored\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70428c07a4924afd931b28c14ed211cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[QUERY] stopwords_and_stemming:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af216d9bfa99429cbfb5313c64934a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[QUERY] stopwords_removed:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:39:10.674 [main] WARN org.terrier.structures.FSADocumentIndex -- This index has fields, but FSADocumentIndex is used (which stores fields lengths on disk); If using field-based models such as BM25F, change to index.document.class in the index  properties file to FSAFieldDocumentIndex or FSADocumentIndexInMemFields to support efficient retrieval. If you don't use (e.g.) BM25F, this warning can be ignored\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f95cfbc4934b86bc4bf5293ed4ac9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[QUERY] full_index:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:41:56.008 [main] WARN org.terrier.structures.FSADocumentIndex -- This index has fields, but FSADocumentIndex is used (which stores fields lengths on disk); If using field-based models such as BM25F, change to index.document.class in the index  properties file to FSAFieldDocumentIndex or FSADocumentIndexInMemFields to support efficient retrieval. If you don't use (e.g.) BM25F, this warning can be ignored\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afcb1c88b74465bacc286b4b6631278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[QUERY] full_index_trimmed:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:51:12.857 [main] WARN org.terrier.structures.FSADocumentIndex -- This index has fields, but FSADocumentIndex is used (which stores fields lengths on disk); If using field-based models such as BM25F, change to index.document.class in the index  properties file to FSAFieldDocumentIndex or FSADocumentIndexInMemFields to support efficient retrieval. If you don't use (e.g.) BM25F, this warning can be ignored\n",
      "\n",
      "Query times summary (s):\n",
      "                                     mean        std       min         max\n",
      "stemming_only                   49.270138  24.455601  1.024041  224.537333\n",
      "stopwords_removed_trimmed       15.782448   6.716416  0.139959  114.413834\n",
      "stopwords_and_stemming_trimmed  17.776235   7.497144  0.202209  129.318084\n",
      "stemming_only_trimmed           55.996675  28.652783  1.058083  261.894292\n",
      "stopwords_and_stemming          17.170530   6.967495  0.206625  102.602834\n",
      "stopwords_removed               16.200586   6.862061  0.175834  141.042083\n",
      "full_index                      55.257526  29.110741  0.211250  242.525833\n",
      "full_index_trimmed              53.403363  32.110103  0.667125  945.019667\n"
     ]
    }
   ],
   "source": [
    "query_times = {}\n",
    "\n",
    "# Get all index folders in BASE_IDX\n",
    "index_folders = [d for d in os.listdir(BASE_IDX) if os.path.isdir(os.path.join(BASE_IDX, d))]\n",
    "\n",
    "for name in index_folders:\n",
    "    abs_idx = os.path.abspath(os.path.join(BASE_IDX, name))\n",
    "\n",
    "    idx_ref = abs_idx  # BatchRetrieve will accept the path\n",
    "    retriever = pt.terrier.Retriever(idx_ref, wmodel=\"BM25\")\n",
    "\n",
    "    times = []\n",
    "    for q in tqdm(qs['query'], desc=f\"[QUERY] {name}\"):\n",
    "        iter = pd.DataFrame([[\"1\", q]], columns=[\"qid\", \"query\"]).itertuples()\n",
    "        for row in iter:\n",
    "            t0 = time.perf_counter()\n",
    "            _ = retriever._retrieve_one(row)\n",
    "            times.append((time.perf_counter()-t0)*1000)\n",
    "    query_times[name] = times\n",
    "\n",
    "# summary\n",
    "qt = pd.DataFrame(query_times)\n",
    "print(\"\\nQuery times summary (s):\")\n",
    "print(qt.describe().T[[\"mean\",\"std\",\"min\",\"max\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query times summary (s):\n",
      "                                     mean        std       min         max\n",
      "stemming_only                   49.270138  24.455601  1.024041  224.537333\n",
      "stopwords_removed_trimmed       15.782448   6.716416  0.139959  114.413834\n",
      "stopwords_and_stemming_trimmed  17.776235   7.497144  0.202209  129.318084\n",
      "stemming_only_trimmed           55.996675  28.652783  1.058083  261.894292\n",
      "stopwords_and_stemming          17.170530   6.967495  0.206625  102.602834\n",
      "stopwords_removed               16.200586   6.862061  0.175834  141.042083\n",
      "full_index                      55.257526  29.110741  0.211250  242.525833\n",
      "full_index_trimmed              53.403363  32.110103  0.667125  945.019667\n"
     ]
    }
   ],
   "source": [
    "qt = pd.DataFrame(query_times)\n",
    "print(\"\\nQuery times summary (s):\")\n",
    "print(qt.describe().T[[\"mean\", \"std\",\"min\",\"max\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_index: mean=55.3ms, 95% CI=(54.7, 55.8)\n",
      "full_index_trimmed: mean=53.4ms, 95% CI=(52.8, 54.0)\n",
      "stemming_only: mean=49.3ms, 95% CI=(48.8, 49.7)\n",
      "stemming_only_trimmed: mean=56.0ms, 95% CI=(55.4, 56.6)\n",
      "stopwords_and_stemming: mean=17.2ms, 95% CI=(17.0, 17.3)\n",
      "stopwords_and_stemming_trimmed: mean=17.8ms, 95% CI=(17.6, 17.9)\n",
      "stopwords_removed: mean=16.2ms, 95% CI=(16.1, 16.3)\n",
      "stopwords_removed_trimmed: mean=15.8ms, 95% CI=(15.7, 15.9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "for name in sorted(query_times.keys()):\n",
    "    times = query_times[name]\n",
    "    arr = np.array(times)\n",
    "    n = len(arr)\n",
    "    mean = arr.mean()\n",
    "    sem  = stats.sem(arr)\n",
    "    t_crit = stats.t.ppf(0.975, df=n-1)\n",
    "    h = t_crit * sem\n",
    "    ci = (mean - h, mean + h)\n",
    "    print(f\"{name}: mean={mean:.1f}ms, 95% CI=({ci[0]:.1f}, {ci[1]:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming_only                  vs stopwords_removed_trimmed      → p=0.000, holm p=0.000 ✓, Cohen's d=1.87\n",
      "stemming_only                  vs stopwords_and_stemming_trimmed → p=0.000, holm p=0.000 ✓, Cohen's d=1.74\n",
      "stemming_only                  vs stemming_only_trimmed          → p=0.000, holm p=0.000 ✓, Cohen's d=-0.25\n",
      "stemming_only                  vs stopwords_and_stemming         → p=0.000, holm p=0.000 ✓, Cohen's d=1.79\n",
      "stemming_only                  vs stopwords_removed              → p=0.000, holm p=0.000 ✓, Cohen's d=1.84\n",
      "stemming_only                  vs full_index                     → p=0.000, holm p=0.000 ✓, Cohen's d=-0.22\n",
      "stemming_only                  vs full_index_trimmed             → p=0.000, holm p=0.000 ✓, Cohen's d=-0.14\n",
      "stopwords_removed_trimmed      vs stopwords_and_stemming_trimmed → p=0.000, holm p=0.000 ✓, Cohen's d=-0.28\n",
      "stopwords_removed_trimmed      vs stemming_only_trimmed          → p=0.000, holm p=0.000 ✓, Cohen's d=-1.93\n",
      "stopwords_removed_trimmed      vs stopwords_and_stemming         → p=0.000, holm p=0.000 ✓, Cohen's d=-0.20\n",
      "stopwords_removed_trimmed      vs stopwords_removed              → p=0.000, holm p=0.000 ✓, Cohen's d=-0.06\n",
      "stopwords_removed_trimmed      vs full_index                     → p=0.000, holm p=0.000 ✓, Cohen's d=-1.87\n",
      "stopwords_removed_trimmed      vs full_index_trimmed             → p=0.000, holm p=0.000 ✓, Cohen's d=-1.62\n",
      "stopwords_and_stemming_trimmed vs stemming_only_trimmed          → p=0.000, holm p=0.000 ✓, Cohen's d=-1.83\n",
      "stopwords_and_stemming_trimmed vs stopwords_and_stemming         → p=0.000, holm p=0.000 ✓, Cohen's d=0.08\n",
      "stopwords_and_stemming_trimmed vs stopwords_removed              → p=0.000, holm p=0.000 ✓, Cohen's d=0.22\n",
      "stopwords_and_stemming_trimmed vs full_index                     → p=0.000, holm p=0.000 ✓, Cohen's d=-1.76\n",
      "stopwords_and_stemming_trimmed vs full_index_trimmed             → p=0.000, holm p=0.000 ✓, Cohen's d=-1.53\n",
      "stemming_only_trimmed          vs stopwords_and_stemming         → p=0.000, holm p=0.000 ✓, Cohen's d=1.86\n",
      "stemming_only_trimmed          vs stopwords_removed              → p=0.000, holm p=0.000 ✓, Cohen's d=1.91\n",
      "stemming_only_trimmed          vs full_index                     → p=0.070, holm p=0.070 ✗, Cohen's d=0.03\n",
      "stemming_only_trimmed          vs full_index_trimmed             → p=0.000, holm p=0.000 ✓, Cohen's d=0.09\n",
      "stopwords_and_stemming         vs stopwords_removed              → p=0.000, holm p=0.000 ✓, Cohen's d=0.14\n",
      "stopwords_and_stemming         vs full_index                     → p=0.000, holm p=0.000 ✓, Cohen's d=-1.80\n",
      "stopwords_and_stemming         vs full_index_trimmed             → p=0.000, holm p=0.000 ✓, Cohen's d=-1.56\n",
      "stopwords_removed              vs full_index                     → p=0.000, holm p=0.000 ✓, Cohen's d=-1.85\n",
      "stopwords_removed              vs full_index_trimmed             → p=0.000, holm p=0.000 ✓, Cohen's d=-1.60\n",
      "full_index                     vs full_index_trimmed             → p=0.000, holm p=0.000 ✓, Cohen's d=0.06\n"
     ]
    }
   ],
   "source": [
    "# 1) Pairwise Welch tests\n",
    "pairs = list(itertools.combinations(query_times.keys(), 2))\n",
    "pvals = [stats.ttest_ind(query_times[a], query_times[b], equal_var=False)[1]\n",
    "         for a, b in pairs]\n",
    "\n",
    "# 2) Holm–Bonferroni correction\n",
    "reject_holm, pvals_holm, _, _ = multipletests(pvals, alpha=0.05, method='holm')\n",
    "\n",
    "# 3) Compute Cohen's d for each pair and report\n",
    "for (a, b), p_raw, p_holm, sig in zip(pairs, pvals, pvals_holm, reject_holm):\n",
    "    d = cohens_d(query_times[a], query_times[b])\n",
    "    status = \"✓\" if sig else \"✗\"\n",
    "    print(f\"{a:30s} vs {b:30s} → p={p_raw:.3f}, holm p={p_holm:.3f} {status}, Cohen's d={d:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann-Whitney U p = 3.218e-03\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# collect lists\n",
    "non_trimmed = np.concatenate([\n",
    "    query_times['full_index'],\n",
    "    query_times['stopwords_removed'],\n",
    "    query_times['stemming_only'],\n",
    "    query_times['stopwords_and_stemming']\n",
    "])\n",
    "trimmed = np.concatenate([\n",
    "    query_times['full_index_trimmed'],\n",
    "    query_times['stopwords_removed_trimmed'],\n",
    "    query_times['stemming_only_trimmed'],\n",
    "    query_times['stopwords_and_stemming_trimmed']\n",
    "])\n",
    "\n",
    "\n",
    "stat, p = mannwhitneyu(non_trimmed, trimmed, alternative='two-sided')\n",
    "print(f\"Mann-Whitney U p = {p:.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
