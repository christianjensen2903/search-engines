{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_78844/893728009.py:12: DeprecationWarning: Call to deprecated function (or staticmethod) started. (use pt.java.started() instead) -- Deprecated since version 0.11.0.\n",
      "  if not pt.started():\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytrec_eval\n",
    "import string\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize PyTerrier\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IDX = \"indexes/stopwords_removed\"\n",
    "QUERIES = \"data/train_queries.csv\"\n",
    "QRELS   = \"data/train_qrels.csv\"\n",
    "\n",
    "# Load queries and qrels\n",
    "qs = pd.read_csv(QUERIES, sep=\"\\t\", names=[\"qid\", \"query\"], header=0)\n",
    "qrels = pd.read_csv(QRELS, sep=\"\\t\")\n",
    "\n",
    "# Strip out all punctuation\n",
    "qs['query'] = qs['query'] \\\n",
    "    .str.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Split into train/validation\n",
    "train_qs, val_qs = train_test_split(qs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare qrels dict for pytrec_eval\n",
    "qrels_dict = {}\n",
    "for _, row in qrels.iterrows():\n",
    "    qrels_dict.setdefault(str(row.qid), {})[row[\"docno\"]] = int(row[\"relevance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_78844/3928177286.py:6: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
      "/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_78844/3928177286.py:7: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  lm_dir = pt.BatchRetrieve(index, wmodel=\"DirichletLM\")\n"
     ]
    }
   ],
   "source": [
    "# Build index reference\n",
    "abs_idx_dir = os.path.abspath(BASE_IDX)   # BASE_IDX = \"indexes/stopwords_removed\"\n",
    "index = pt.IndexFactory.of(abs_idx_dir)\n",
    "\n",
    "# Define retrieval models\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "lm_dir = pt.BatchRetrieve(index, wmodel=\"DirichletLM\")\n",
    "\n",
    "# Parameter grids\n",
    "bm25_params = {\n",
    "    'bm25.k_1': [0.5, 1.0, 1.5, 2.0],\n",
    "    'bm25.b':   [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "lm_params = {\n",
    "    'DirichletLM.mu': [500, 1000, 1500, 2000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(run_df, qrels_dict):\n",
    "    \"\"\"\n",
    "    Evaluate a run (DataFrame with qid, docno, score) against qrels_dict\n",
    "    using pytrec_eval, returning a dict of metrics per query.\n",
    "    \"\"\"\n",
    "    # Format for pytrec_eval\n",
    "    run = {}\n",
    "    for qid, group in run_df.groupby('qid'):\n",
    "        run[qid] = {doc: float(score) for doc, score in zip(group['docno'], group['score'])}\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "        qrels_dict, {'ndcg_cut.5','ndcg_cut.10','ndcg_cut.20',\n",
    "                     'recip_rank','P.5','P.10','P.20',\n",
    "                     'recall.5','recall.10','recall.20'}\n",
    "    )\n",
    "    results = evaluator.evaluate(run)\n",
    "    # Aggregate\n",
    "    metrics = {m: np.mean([results[q][m] for q in results]) for m in next(iter(results.values())).keys()}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k1:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "bm25_results = []\n",
    "for k1 in tqdm(bm25_params['bm25.k_1'], desc='k1'):\n",
    "    for b in tqdm(bm25_params['bm25.b'], desc='b', leave=False):\n",
    "        # create a fresh BM25 retriever with your chosen k1 and b\n",
    "        model = pt.terrier.Retriever(\n",
    "            index,\n",
    "            wmodel=\"BM25\",\n",
    "            controls={\n",
    "                \"bm25.k_1\": k1,\n",
    "                \"bm25.b\":   b\n",
    "            }\n",
    "        )\n",
    "        start = time.time()\n",
    "        run = model.transform(train_qs[['qid','query']])\n",
    "        metrics = evaluate(run, qrels_dict)\n",
    "        elapsed = (time.time() - start) / len(train_qs)\n",
    "        bm25_results.append({\n",
    "            'model': 'BM25',\n",
    "            'k1': k1, 'b': b,\n",
    "            **metrics,\n",
    "            'avg_eval_time_s': elapsed\n",
    "        })\n",
    "\n",
    "bm25_df = pd.DataFrame(bm25_results)\n",
    "best_bm25 = bm25_df.sort_values('ndcg_cut_10', ascending=False).iloc[0]\n",
    "best_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_results = []\n",
    "for mu in tqdm(lm_params['DirichletLM.mu'], desc='mu'):\n",
    "    model = pt.terrier.Retriever(\n",
    "        index,\n",
    "        wmodel=\"DirichletLM\",\n",
    "        controls={\n",
    "            \"mu\": mu\n",
    "        }\n",
    "    )\n",
    "    start = time.time()\n",
    "    run = model.transform(train_qs[['qid','query']])\n",
    "    metrics = evaluate(run, qrels_dict)\n",
    "    elapsed = (time.time() - start) / len(train_qs)\n",
    "    lm_results.append({\n",
    "        'model': 'DirichletLM',\n",
    "        'mu': mu,\n",
    "        **metrics,\n",
    "        'avg_eval_time_s': elapsed\n",
    "    })\n",
    "\n",
    "lm_df = pd.DataFrame(lm_results)\n",
    "best_lm = lm_df.sort_values('ndcg_cut_10', ascending=False).iloc[0]\n",
    "best_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = [\n",
    "    (pt.BatchRetrieve(index, wmodel=\"BM25\", bm25_k_1=best_bm25.k1, bm25_b=best_bm25.b), 'BM25'),\n",
    "    (pt.BatchRetrieve(index, wmodel=\"DirichletLM\", DirichletLM_mu=int(best_lm.mu)), 'DirichletLM')\n",
    "]\n",
    "\n",
    "val_results = []\n",
    "for model, name in best_models:\n",
    "    start = time.time()\n",
    "    run = model.transform(val_qs)\n",
    "    metrics = evaluate(run, qrels_dict)\n",
    "    elapsed = (time.time() - start) / len(val_qs)\n",
    "    val_results.append({\n",
    "        'model': name,\n",
    "        **metrics,\n",
    "        'avg_eval_time_s': elapsed\n",
    "    })\n",
    "\n",
    "val_df = pd.DataFrame(val_results)\n",
    "\n",
    "# Combine and display\n",
    "print(\"=== BM25 Grid Search (train) ===\")\n",
    "print(bm25_df.sort_values('ndcg_cut.10', ascending=False).reset_index(drop=True))\n",
    "print(\"\\n=== Dirichlet LM Grid Search (train) ===\")\n",
    "print(lm_df.sort_values('ndcg_cut.10', ascending=False).reset_index(drop=True))\n",
    "print(\"\\nBest BM25 params:\", best_bm25.to_dict())\n",
    "print(\"Best LM params:\", best_lm.to_dict())\n",
    "print(\"\\n=== Validation Results ===\")\n",
    "print(val_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
