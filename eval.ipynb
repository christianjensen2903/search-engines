{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_78844/893728009.py:12: DeprecationWarning: Call to deprecated function (or staticmethod) started. (use pt.java.started() instead) -- Deprecated since version 0.11.0.\n",
      "  if not pt.started():\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytrec_eval\n",
    "import string\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize PyTerrier\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IDX = \"indexes/stopwords_removed\"\n",
    "QUERIES = \"data/train_queries.csv\"\n",
    "QRELS   = \"data/train_qrels.csv\"\n",
    "\n",
    "# Load queries and qrels\n",
    "qs = pd.read_csv(QUERIES, sep=\"\\t\", names=[\"qid\", \"query\"], header=0)\n",
    "qrels = pd.read_csv(QRELS, sep=\"\\t\")\n",
    "\n",
    "# Strip out all punctuation\n",
    "qs['query'] = qs['query'] \\\n",
    "    .str.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Split into train/validation\n",
    "train_qs, val_qs = train_test_split(qs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare qrels dict for pytrec_eval\n",
    "qrels_dict = {}\n",
    "for _, row in qrels.iterrows():\n",
    "    qrels_dict.setdefault(str(row.qid), {})[row[\"docno\"]] = int(row[\"relevance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_78844/3928177286.py:6: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
      "/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_78844/3928177286.py:7: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  lm_dir = pt.BatchRetrieve(index, wmodel=\"DirichletLM\")\n"
     ]
    }
   ],
   "source": [
    "# Build index reference\n",
    "abs_idx_dir = os.path.abspath(BASE_IDX)   # BASE_IDX = \"indexes/stopwords_removed\"\n",
    "index = pt.IndexFactory.of(abs_idx_dir)\n",
    "\n",
    "# Define retrieval models\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "lm_dir = pt.BatchRetrieve(index, wmodel=\"DirichletLM\")\n",
    "\n",
    "# Parameter grids\n",
    "bm25_params = {\n",
    "    'bm25.k_1': [1.0, 1.25, 1.5, 1.75, 2.0],\n",
    "    'bm25.b':   [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "lm_params = {\n",
    "    'DirichletLM.mu': [500, 1000, 1500, 2000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(run_df, qrels_dict):\n",
    "    \"\"\"\n",
    "    Evaluate a run (DataFrame with qid, docno, score) against qrels_dict\n",
    "    using pytrec_eval, returning a dict of metrics per query.\n",
    "    \"\"\"\n",
    "    # Format for pytrec_eval\n",
    "    run = {}\n",
    "    for qid, group in run_df.groupby('qid'):\n",
    "        run[qid] = {doc: float(score) for doc, score in zip(group['docno'], group['score'])}\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "        qrels_dict, {'ndcg_cut.5','ndcg_cut.10','ndcg_cut.20',\n",
    "                     'recip_rank','P.5','P.10','P.20',\n",
    "                     'recall.5','recall.10','recall.20'}\n",
    "    )\n",
    "    results = evaluator.evaluate(run)\n",
    "    # Aggregate\n",
    "    metrics = {m: np.mean([results[q][m] for q in results]) for m in next(iter(results.values())).keys()}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k1: 100%|██████████| 4/4 [52:50<00:00, 792.52s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model                  BM25\n",
       "k1                      2.0\n",
       "b                      0.75\n",
       "recip_rank          0.21382\n",
       "P_5                0.065715\n",
       "P_10               0.039266\n",
       "P_20               0.020847\n",
       "recall_5           0.328577\n",
       "recall_10          0.392665\n",
       "recall_20          0.416948\n",
       "ndcg_cut_5         0.234097\n",
       "ndcg_cut_10        0.255082\n",
       "ndcg_cut_20        0.261284\n",
       "avg_eval_time_s     0.01726\n",
       "Name: 18, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_results = []\n",
    "for k1 in tqdm(bm25_params['bm25.k_1'], desc='k1'):\n",
    "    for b in tqdm(bm25_params['bm25.b'], desc='b', leave=False):\n",
    "        # create a fresh BM25 retriever with your chosen k1 and b\n",
    "        model = pt.terrier.Retriever(\n",
    "            index,\n",
    "            wmodel=\"BM25\",\n",
    "            controls={\n",
    "                \"bm25.k_1\": k1,\n",
    "                \"bm25.b\":   b\n",
    "            }\n",
    "        )\n",
    "        start = time.time()\n",
    "        run = model.transform(train_qs[['qid','query']])\n",
    "        metrics = evaluate(run, qrels_dict)\n",
    "        elapsed = (time.time() - start) / len(train_qs)\n",
    "        bm25_results.append({\n",
    "            'model': 'BM25',\n",
    "            'k1': k1, 'b': b,\n",
    "            **metrics,\n",
    "            'avg_eval_time_s': elapsed\n",
    "        })\n",
    "\n",
    "bm25_df = pd.DataFrame(bm25_results)\n",
    "best_bm25 = bm25_df.sort_values('ndcg_cut_10', ascending=False).iloc[0]\n",
    "best_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mu: 100%|██████████| 4/4 [10:05<00:00, 151.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model              DirichletLM\n",
       "mu                         500\n",
       "recip_rank            0.177736\n",
       "P_5                   0.055802\n",
       "P_10                  0.037276\n",
       "P_20                   0.02056\n",
       "recall_5              0.279009\n",
       "recall_10             0.372763\n",
       "recall_20              0.41119\n",
       "ndcg_cut_5            0.190705\n",
       "ndcg_cut_10            0.22117\n",
       "ndcg_cut_20            0.23111\n",
       "avg_eval_time_s       0.019782\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_results = []\n",
    "for mu in tqdm(lm_params['DirichletLM.mu'], desc='mu'):\n",
    "    model = pt.terrier.Retriever(\n",
    "        index,\n",
    "        wmodel=\"DirichletLM\",\n",
    "        controls={\n",
    "            \"mu\": mu\n",
    "        }\n",
    "    )\n",
    "    start = time.time()\n",
    "    run = model.transform(train_qs[['qid','query']])\n",
    "    metrics = evaluate(run, qrels_dict)\n",
    "    elapsed = (time.time() - start) / len(train_qs)\n",
    "    lm_results.append({\n",
    "        'model': 'DirichletLM',\n",
    "        'mu': mu,\n",
    "        **metrics,\n",
    "        'avg_eval_time_s': elapsed\n",
    "    })\n",
    "\n",
    "lm_df = pd.DataFrame(lm_results)\n",
    "best_lm = lm_df.sort_values('ndcg_cut_10', ascending=False).iloc[0]\n",
    "best_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This should be done for all the indices with stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_78844/566822781.py:3: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  pt.BatchRetrieve(\n",
      "/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_78844/566822781.py:14: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  pt.BatchRetrieve(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BM25 Grid Search (train) ===\n",
      "   model   k1     b  recip_rank       P_5      P_10      P_20  recall_5  \\\n",
      "0   BM25  2.0  0.75    0.213820  0.065715  0.039266  0.020847  0.328577   \n",
      "1   BM25  1.5  0.75    0.213096  0.065115  0.039191  0.020747  0.325573   \n",
      "2   BM25  2.0  1.00    0.211604  0.064614  0.038803  0.020635  0.323069   \n",
      "3   BM25  1.0  0.75    0.210496  0.063988  0.038678  0.020585  0.319940   \n",
      "4   BM25  1.5  1.00    0.210790  0.064113  0.038578  0.020603  0.320566   \n",
      "5   BM25  1.0  1.00    0.207635  0.063537  0.038190  0.020459  0.317687   \n",
      "6   BM25  2.0  0.50    0.204821  0.063838  0.038929  0.020772  0.319189   \n",
      "7   BM25  1.5  0.50    0.204303  0.063638  0.038891  0.020697  0.318188   \n",
      "8   BM25  0.5  0.75    0.204767  0.062160  0.037564  0.020146  0.310802   \n",
      "9   BM25  1.0  0.50    0.201846  0.062486  0.038403  0.020522  0.312430   \n",
      "10  BM25  0.5  1.00    0.203525  0.061910  0.037351  0.020084  0.309551   \n",
      "11  BM25  0.5  0.50    0.196933  0.060859  0.037264  0.020071  0.304293   \n",
      "12  BM25  1.5  0.25    0.187133  0.059231  0.037739  0.020403  0.296157   \n",
      "13  BM25  2.0  0.25    0.185710  0.059282  0.037915  0.020491  0.296408   \n",
      "14  BM25  1.0  0.25    0.185839  0.059031  0.037226  0.020222  0.295156   \n",
      "15  BM25  0.5  0.25    0.181415  0.057204  0.036375  0.019727  0.286018   \n",
      "16  BM25  2.0  0.00    0.142374  0.045563  0.031769  0.018757  0.227813   \n",
      "17  BM25  1.5  0.00    0.142718  0.045538  0.031656  0.018738  0.227688   \n",
      "18  BM25  1.0  0.00    0.142580  0.045738  0.031255  0.018538  0.228689   \n",
      "19  BM25  0.5  0.00    0.140079  0.044937  0.030580  0.018012  0.224684   \n",
      "\n",
      "    recall_10  recall_20  ndcg_cut_5  ndcg_cut_10  ndcg_cut_20  \\\n",
      "0    0.392665   0.416948    0.234097     0.255082     0.261284   \n",
      "1    0.391914   0.414946    0.232642     0.254346     0.260216   \n",
      "2    0.388034   0.412692    0.230841     0.252107     0.258440   \n",
      "3    0.386782   0.411691    0.229045     0.250942     0.257314   \n",
      "4    0.385780   0.412067    0.229538     0.250869     0.257608   \n",
      "5    0.381900   0.409188    0.226445     0.247433     0.254446   \n",
      "6    0.389285   0.415446    0.224141     0.247175     0.253906   \n",
      "7    0.388910   0.413944    0.223571     0.246683     0.253104   \n",
      "8    0.375642   0.402929    0.222406     0.243666     0.250639   \n",
      "9    0.384028   0.410439    0.220078     0.243478     0.250267   \n",
      "10   0.373514   0.401677    0.221238     0.242096     0.249287   \n",
      "11   0.372637   0.401427    0.214409     0.236817     0.244170   \n",
      "12   0.377394   0.408061    0.203567     0.230244     0.238133   \n",
      "13   0.379146   0.409813    0.202482     0.229574     0.237452   \n",
      "14   0.372262   0.404431    0.202716     0.227904     0.236211   \n",
      "15   0.363750   0.394542    0.196973     0.222406     0.230323   \n",
      "16   0.317687   0.375141    0.150389     0.179397     0.194093   \n",
      "17   0.316560   0.374765    0.150664     0.179380     0.194303   \n",
      "18   0.312555   0.370760    0.151228     0.178344     0.193260   \n",
      "19   0.305795   0.360245    0.148658     0.174964     0.188907   \n",
      "\n",
      "    avg_eval_time_s  \n",
      "0          0.017260  \n",
      "1          0.022342  \n",
      "2          0.017572  \n",
      "3          0.016509  \n",
      "4          0.020788  \n",
      "5          0.017124  \n",
      "6          0.016966  \n",
      "7          0.019786  \n",
      "8          0.024607  \n",
      "9          0.016536  \n",
      "10         0.022471  \n",
      "11         0.022464  \n",
      "12         0.018699  \n",
      "13         0.018496  \n",
      "14         0.017305  \n",
      "15         0.020153  \n",
      "16         0.019585  \n",
      "17         0.024015  \n",
      "18         0.021468  \n",
      "19         0.022081  \n",
      "\n",
      "=== Dirichlet LM Grid Search (train) ===\n",
      "         model    mu  recip_rank       P_5      P_10     P_20  recall_5  \\\n",
      "0  DirichletLM   500    0.177736  0.055802  0.037276  0.02056  0.279009   \n",
      "1  DirichletLM  1000    0.177736  0.055802  0.037276  0.02056  0.279009   \n",
      "2  DirichletLM  1500    0.177736  0.055802  0.037276  0.02056  0.279009   \n",
      "3  DirichletLM  2000    0.177736  0.055802  0.037276  0.02056  0.279009   \n",
      "\n",
      "   recall_10  recall_20  ndcg_cut_5  ndcg_cut_10  ndcg_cut_20  avg_eval_time_s  \n",
      "0   0.372763    0.41119    0.190705      0.22117      0.23111         0.019782  \n",
      "1   0.372763    0.41119    0.190705      0.22117      0.23111         0.019313  \n",
      "2   0.372763    0.41119    0.190705      0.22117      0.23111         0.018466  \n",
      "3   0.372763    0.41119    0.190705      0.22117      0.23111         0.018066  \n",
      "\n",
      "Best BM25 params: {'model': 'BM25', 'k1': 2.0, 'b': 0.75, 'recip_rank': 0.21382028469437123, 'P_5': 0.06571535861809989, 'P_10': 0.03926649142571034, 'P_20': 0.020847415195894354, 'recall_5': 0.3285767930904994, 'recall_10': 0.39266491425710354, 'recall_20': 0.4169483039178871, 'ndcg_cut_5': 0.2340972946936316, 'ndcg_cut_10': 0.2550821926782679, 'ndcg_cut_20': 0.26128363516275377, 'avg_eval_time_s': 0.017259905397892}\n",
      "Best LM params: {'model': 'DirichletLM', 'mu': 500, 'recip_rank': 0.1777358025418005, 'P_5': 0.05580172737514081, 'P_10': 0.03727625485041932, 'P_20': 0.02055951933909125, 'recall_5': 0.2790086368757041, 'recall_10': 0.37276254850419327, 'recall_20': 0.411190386781825, 'ndcg_cut_5': 0.19070508163915503, 'ndcg_cut_10': 0.221169511466555, 'ndcg_cut_20': 0.2311099974796305, 'avg_eval_time_s': 0.019782269477844238}\n",
      "\n",
      "=== Validation Results ===\n",
      "         model  recip_rank       P_5      P_10      P_20  recall_5  recall_10  \\\n",
      "0         BM25    0.217734  0.064497  0.039009  0.020806  0.322484   0.390085   \n",
      "1  DirichletLM    0.204715  0.061592  0.038858  0.020706  0.307962   0.388583   \n",
      "\n",
      "   recall_20  ndcg_cut_5  ndcg_cut_10  ndcg_cut_20  avg_eval_time_s  \n",
      "0   0.416124    0.234951     0.257108     0.263788         0.018314  \n",
      "1   0.414121    0.220218     0.246553     0.253174         0.017686  \n"
     ]
    }
   ],
   "source": [
    "best_models = [\n",
    "    (\n",
    "        pt.BatchRetrieve(\n",
    "            index,\n",
    "            wmodel=\"BM25\",\n",
    "            controls={\n",
    "                \"bm25.k_1\": best_bm25[\"k1\"],\n",
    "                \"bm25.b\":  best_bm25[\"b\"]\n",
    "            }\n",
    "        ),\n",
    "        \"BM25\"\n",
    "    ),\n",
    "    (\n",
    "        pt.BatchRetrieve(\n",
    "            index,\n",
    "            wmodel=\"DirichletLM\",\n",
    "            controls={\n",
    "                \"dirichletlm.mu\": int(best_lm[\"mu\"])\n",
    "            }\n",
    "        ),\n",
    "        \"DirichletLM\"\n",
    "    )\n",
    "]\n",
    "\n",
    "val_results = []\n",
    "for model, name in best_models:\n",
    "    start = time.time()\n",
    "    run = model.transform(val_qs)\n",
    "    metrics = evaluate(run, qrels_dict)\n",
    "    elapsed = (time.time() - start) / len(val_qs)\n",
    "    val_results.append({\n",
    "        'model': name,\n",
    "        **metrics,\n",
    "        'avg_eval_time_s': elapsed\n",
    "    })\n",
    "\n",
    "val_df = pd.DataFrame(val_results)\n",
    "\n",
    "# Combine and display\n",
    "print(\"=== BM25 Grid Search (train) ===\")\n",
    "print(bm25_df.sort_values('ndcg_cut_10', ascending=False).reset_index(drop=True))\n",
    "print(\"\\n=== Dirichlet LM Grid Search (train) ===\")\n",
    "print(lm_df.sort_values('ndcg_cut_10', ascending=False).reset_index(drop=True))\n",
    "print(\"\\nBest BM25 params:\", best_bm25.to_dict())\n",
    "print(\"Best LM params:\", best_lm.to_dict())\n",
    "print(\"\\n=== Validation Results ===\")\n",
    "print(val_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
